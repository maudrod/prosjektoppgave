AstridsModified.py:228: RuntimeWarning: overflow encountered in exp
  p_old = np.sum([np.exp(a) for a in log_lik_list_scaled])
AstridsModified.py:185: RuntimeWarning: divide by zero encountered in double_scalars
  new_alpha = (m**2)/v
AstridsModified.py:193: RuntimeWarning: divide by zero encountered in double_scalars
  new_alpha = (m**2)/v
0.005 20
0.005 20
0.005 20
0.005 20
0.005 20
0.005 20
0.005 20
0.005 20
0.005 20
0.005 20
0.005 20
0.005 20
0.005 20
0.005 20
0.005 20
0.005 20
0.005 20
0.005 20
0.005 20
0.005 20
0.005 20
0.005 20
0.005 20
0.005 20
0.005 20
0.005 20
0.005 20
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
0.004207068775388316 23.9033270732949
Traceback (most recent call last):
  File "AstridsModified.py", line 317, in <module>
    (A_list,tau_list,c) = particle_marginal_Metropolis_Hastings_both(A_p_start,tau_start, W_0, b1, b2, iterations,alpha, alpha_tau)
  File "AstridsModified.py", line 256, in particle_marginal_Metropolis_Hastings_both
    w_p_new, v_p_new = resample_weights(w_p_new,v_p_new,P)
  File "AstridsModified.py", line 94, in resample_weights
    custm = scipy.stats.rv_discrete(name='custm', values=(xk, pk)) 
  File "/lustre1/work/maudr/Prosjektoppgave/datasci/lib/python3.8/site-packages/scipy/stats/_distn_infrastructure.py", line 3529, in __init__
    raise ValueError("The sum of provided pk is not 1.")
ValueError: The sum of provided pk is not 1.
